{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/GeSpjOvKgGxidP/K5sdr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/twyeh/AI-in-education/blob/main/SNN_for_inclined_projectile_motion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-6rOOKSiVO6",
        "outputId": "b42a8229-9173-46ba-df9e-f343f08ba2d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 13121.7490 - val_loss: 5122.1592\n",
            "Epoch 2/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4101.0425 - val_loss: 2766.4590\n",
            "Epoch 3/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2566.2952 - val_loss: 1912.4510\n",
            "Epoch 4/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1723.9648 - val_loss: 1328.9990\n",
            "Epoch 5/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1239.6406 - val_loss: 1034.0287\n",
            "Epoch 6/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 985.1047 - val_loss: 882.7141\n",
            "Epoch 7/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 863.3345 - val_loss: 783.1308\n",
            "Epoch 8/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 737.5663 - val_loss: 687.7902\n",
            "Epoch 9/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 661.5458 - val_loss: 611.9919\n",
            "Epoch 10/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 613.3434 - val_loss: 560.1088\n",
            "Epoch 11/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 542.9087 - val_loss: 516.1658\n",
            "Epoch 12/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 504.6916 - val_loss: 488.7378\n",
            "Epoch 13/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 482.6937 - val_loss: 451.2300\n",
            "Epoch 14/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 436.6402 - val_loss: 417.3919\n",
            "Epoch 15/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 421.6404 - val_loss: 402.9252\n",
            "Epoch 16/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 393.1775 - val_loss: 401.2054\n",
            "Epoch 17/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 380.2171 - val_loss: 374.0115\n",
            "Epoch 18/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 361.9955 - val_loss: 349.8828\n",
            "Epoch 19/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 347.9585 - val_loss: 337.1704\n",
            "Epoch 20/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 340.5209 - val_loss: 335.0955\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
            "預測位置 (x, y): [57.6139   36.549835]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input # Import Input layer\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# 產生斜拋運動的訓練資料\n",
        "def generate_data(num_samples=10000):\n",
        "    g = 9.8  # 重力加速度\n",
        "    v0 = np.random.uniform(10, 50, num_samples)           # 初速度 (10~50 m/s)\n",
        "    theta = np.random.uniform(0, np.pi/2, num_samples)    # 拋射角度 (0~90度, 轉弧度)\n",
        "    t = np.random.uniform(0, 10, num_samples)             # 時間 (0~10秒)\n",
        "\n",
        "    x = v0 * np.cos(theta) * t\n",
        "    y = v0 * np.sin(theta) * t - 0.5 * g * t**2\n",
        "\n",
        "    # 輸入為 time, v0, theta 三個變數\n",
        "    X = np.stack([t, v0, theta], axis=1)\n",
        "    # 輸出為 x, y 兩個位置值\n",
        "    Y = np.stack([x, y], axis=1)\n",
        "\n",
        "    return X, Y\n",
        "\n",
        "# 建立神經網路模型\n",
        "def build_model():\n",
        "    model = Sequential([\n",
        "        Input(shape=(3,)),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(2)  # 輸出x和y坐標\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "    return model\n",
        "\n",
        "# 產生資料\n",
        "X_train, Y_train = generate_data(20000)\n",
        "\n",
        "# 創建模型\n",
        "model = build_model()\n",
        "\n",
        "# 訓練模型\n",
        "model.fit(X_train, Y_train, epochs=20, batch_size=64, validation_split=0.2)\n",
        "\n",
        "# 測試：輸入時間為2秒，初速度50m/s，角度45度(弧度0.785)\n",
        "test_input = np.array([[2.0, 50.0, np.pi/4]])\n",
        "predicted_position = model.predict(test_input)\n",
        "print(f\"預測位置 (x, y): {predicted_position[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2227b00f",
        "outputId": "04e1e092-d575-45ee-f21a-67b5693781b5"
      },
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Evaluate on training and validation data using MAE\n",
        "train_preds = model.predict(X_train)\n",
        "mae_train = mean_absolute_error(Y_train, train_preds)\n",
        "print(f\"Mean Absolute Error on training data: {mae_train:.4f}\")\n",
        "\n",
        "# Assuming validation data is available from the training split\n",
        "# We can split the training data again to get validation data for evaluation\n",
        "split_index = int(len(X_train) * 0.8)\n",
        "X_val, Y_val = X_train[split_index:], Y_train[split_index:]\n",
        "\n",
        "val_preds = model.predict(X_val)\n",
        "mae_val = mean_absolute_error(Y_val, val_preds)\n",
        "print(f\"Mean Absolute Error on validation data: {mae_val:.4f}\")\n",
        "\n",
        "# Test with a few known examples\n",
        "test_cases = [\n",
        "    [2.0, 50.0, np.pi/4],  # t=2, v0=50, theta=45 deg\n",
        "    [3.0, 30.0, np.pi/6],  # t=3, v0=30, theta=30 deg\n",
        "    [1.5, 40.0, np.pi/3]   # t=1.5, v0=40, theta=60 deg\n",
        "]\n",
        "\n",
        "print(\"\\nTesting with known examples:\")\n",
        "for t, v0, theta in test_cases:\n",
        "    # Calculate true position\n",
        "    x_true = v0 * np.cos(theta) * t\n",
        "    y_true = v0 * np.sin(theta) * t - 0.5 * 9.8 * t**2\n",
        "\n",
        "    # Predict with the model\n",
        "    test_input = np.array([[t, v0, theta]])\n",
        "    predicted_position = model.predict(test_input, verbose=0)[0]\n",
        "\n",
        "    print(f\"Input: t={t}, v0={v0}, theta={np.degrees(theta):.2f} deg\")\n",
        "    print(f\"True position (x, y): ({x_true:.2f}, {y_true:.2f})\")\n",
        "    print(f\"Predicted position (x, y): ({predicted_position[0]:.2f}, {predicted_position[1]:.2f})\")\n",
        "    print(\"-\" * 20)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Mean Absolute Error on training data: 13.3458\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
            "Mean Absolute Error on validation data: 13.4419\n",
            "\n",
            "Testing with known examples:\n",
            "Input: t=2.0, v0=50.0, theta=45.00 deg\n",
            "True position (x, y): (70.71, 51.11)\n",
            "Predicted position (x, y): (57.61, 36.55)\n",
            "--------------------\n",
            "Input: t=3.0, v0=30.0, theta=30.00 deg\n",
            "True position (x, y): (77.94, 0.90)\n",
            "Predicted position (x, y): (66.37, 20.29)\n",
            "--------------------\n",
            "Input: t=1.5, v0=40.0, theta=60.00 deg\n",
            "True position (x, y): (30.00, 40.94)\n",
            "Predicted position (x, y): (23.13, 38.98)\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bbb0447"
      },
      "source": [
        "# Task\n",
        "Tune the hyperparameters of the model to improve its performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb63f17c"
      },
      "source": [
        "## Define hyperparameter search space\n",
        "\n",
        "### Subtask:\n",
        "Specify the range of values or options to explore for hyperparameters like learning rate, number of layers, neurons per layer, and activation functions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6845ad5"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the hyperparameter search space as a dictionary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3abe974f",
        "outputId": "b191efd3-9197-498a-fc00-3a42534cda68"
      },
      "source": [
        "# Define the hyperparameter search space\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.001, 0.0001],\n",
        "    'num_layers': [1, 2, 3],\n",
        "    'neurons_per_layer': {\n",
        "        1: [32, 64, 128],\n",
        "        2: [(32, 32), (64, 32), (64, 64), (128, 64)],\n",
        "        3: [(32, 32, 32), (64, 32, 16), (128, 64, 32)]\n",
        "    },\n",
        "    'activation': ['relu', 'tanh']\n",
        "}\n",
        "\n",
        "print(param_grid)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'learning_rate': [0.01, 0.001, 0.0001], 'num_layers': [1, 2, 3], 'neurons_per_layer': {1: [32, 64, 128], 2: [(32, 32), (64, 32), (64, 64), (128, 64)], 3: [(32, 32, 32), (64, 32, 16), (128, 64, 32)]}, 'activation': ['relu', 'tanh']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "515d8507"
      },
      "source": [
        "## Implement hyperparameter tuning\n",
        "\n",
        "### Subtask:\n",
        "Use a library or custom code to systematically train and evaluate the model with different hyperparameter combinations. This could involve techniques like grid search, random search, or more advanced methods like Bayesian optimization.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ba8c774"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a function to build the model with specified hyperparameters, generate and split the data, and iterate through hyperparameter combinations to train and evaluate models, keeping track of the best one.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ecd5ac9",
        "outputId": "2643028c-0d5e-447c-a062-8559859999dd"
      },
      "source": [
        "def build_tunable_model(num_layers, neurons_per_layer, activation, learning_rate):\n",
        "    \"\"\"Builds a Keras Sequential model with specified hyperparameters.\"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(3,))) # Input layer\n",
        "\n",
        "    if num_layers == 1:\n",
        "        model.add(Dense(neurons_per_layer, activation=activation))\n",
        "    elif num_layers > 1:\n",
        "        for i, neurons in enumerate(neurons_per_layer):\n",
        "            model.add(Dense(neurons, activation=activation))\n",
        "\n",
        "    model.add(Dense(2))  # Output layer for x, y\n",
        "\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "# Generate data\n",
        "X, Y = generate_data(20000)\n",
        "\n",
        "# Split data\n",
        "split_ratio = 0.8\n",
        "split_index = int(len(X) * split_ratio)\n",
        "X_train, X_val = X[:split_index], X[split_index:]\n",
        "Y_train, Y_val = Y[:split_index], Y[split_index:]\n",
        "\n",
        "best_mae = float('inf')\n",
        "best_params = None\n",
        "history_list = []\n",
        "\n",
        "# Iterate through hyperparameter combinations\n",
        "for lr in param_grid['learning_rate']:\n",
        "    for num_layers in param_grid['num_layers']:\n",
        "        neurons_options = param_grid['neurons_per_layer'][num_layers]\n",
        "        if num_layers == 1:\n",
        "             # neurons_options is a list of integers for num_layers == 1\n",
        "             for neurons in neurons_options:\n",
        "                for activation in param_grid['activation']:\n",
        "                    print(f\"Training with: LR={lr}, Layers={num_layers}, Neurons={neurons}, Activation={activation}\")\n",
        "                    model = build_tunable_model(num_layers, neurons, activation, lr)\n",
        "                    history = model.fit(X_train, Y_train, epochs=20, batch_size=64, validation_data=(X_val, Y_val), verbose=0)\n",
        "                    val_mae = history.history['val_mae'][-1]\n",
        "                    print(f\"Validation MAE: {val_mae:.4f}\")\n",
        "\n",
        "                    if val_mae < best_mae:\n",
        "                        best_mae = val_mae\n",
        "                        best_params = {'learning_rate': lr, 'num_layers': num_layers, 'neurons': neurons, 'activation': activation}\n",
        "\n",
        "                    history_list.append({'params': {'learning_rate': lr, 'num_layers': num_layers, 'neurons': neurons, 'activation': activation},\n",
        "                                         'history': history.history})\n",
        "        else:\n",
        "            # neurons_options is a list of tuples for num_layers > 1\n",
        "            for neurons in neurons_options:\n",
        "                for activation in param_grid['activation']:\n",
        "                    print(f\"Training with: LR={lr}, Layers={num_layers}, Neurons={neurons}, Activation={activation}\")\n",
        "                    model = build_tunable_model(num_layers, neurons, activation, lr)\n",
        "                    history = model.fit(X_train, Y_train, epochs=20, batch_size=64, validation_data=(X_val, Y_val), verbose=0)\n",
        "                    val_mae = history.history['val_mae'][-1]\n",
        "                    print(f\"Validation MAE: {val_mae:.4f}\")\n",
        "\n",
        "                    if val_mae < best_mae:\n",
        "                        best_mae = val_mae\n",
        "                        best_params = {'learning_rate': lr, 'num_layers': num_layers, 'neurons': neurons, 'activation': activation}\n",
        "\n",
        "                    history_list.append({'params': {'learning_rate': lr, 'num_layers': num_layers, 'neurons': neurons, 'activation': activation},\n",
        "                                         'history': history.history})\n",
        "\n",
        "\n",
        "print(\"\\nBest hyperparameters found:\")\n",
        "print(best_params)\n",
        "print(f\"Best Validation MAE: {best_mae:.4f}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with: LR=0.01, Layers=1, Neurons=32, Activation=relu\n",
            "Validation MAE: 10.2303\n",
            "Training with: LR=0.01, Layers=1, Neurons=32, Activation=tanh\n",
            "Validation MAE: 10.8349\n",
            "Training with: LR=0.01, Layers=1, Neurons=64, Activation=relu\n",
            "Validation MAE: 8.7818\n",
            "Training with: LR=0.01, Layers=1, Neurons=64, Activation=tanh\n",
            "Validation MAE: 6.1706\n",
            "Training with: LR=0.01, Layers=1, Neurons=128, Activation=relu\n",
            "Validation MAE: 5.9379\n",
            "Training with: LR=0.01, Layers=1, Neurons=128, Activation=tanh\n",
            "Validation MAE: 3.5610\n",
            "Training with: LR=0.01, Layers=2, Neurons=(32, 32), Activation=relu\n",
            "Validation MAE: 4.1487\n",
            "Training with: LR=0.01, Layers=2, Neurons=(32, 32), Activation=tanh\n",
            "Validation MAE: 3.3994\n",
            "Training with: LR=0.01, Layers=2, Neurons=(64, 32), Activation=relu\n",
            "Validation MAE: 4.1484\n",
            "Training with: LR=0.01, Layers=2, Neurons=(64, 32), Activation=tanh\n",
            "Validation MAE: 5.6846\n",
            "Training with: LR=0.01, Layers=2, Neurons=(64, 64), Activation=relu\n",
            "Validation MAE: 3.1737\n",
            "Training with: LR=0.01, Layers=2, Neurons=(64, 64), Activation=tanh\n",
            "Validation MAE: 3.1205\n",
            "Training with: LR=0.01, Layers=2, Neurons=(128, 64), Activation=relu\n",
            "Validation MAE: 2.8433\n",
            "Training with: LR=0.01, Layers=2, Neurons=(128, 64), Activation=tanh\n",
            "Validation MAE: 4.7158\n",
            "Training with: LR=0.01, Layers=3, Neurons=(32, 32, 32), Activation=relu\n",
            "Validation MAE: 3.1894\n",
            "Training with: LR=0.01, Layers=3, Neurons=(32, 32, 32), Activation=tanh\n",
            "Validation MAE: 4.7673\n",
            "Training with: LR=0.01, Layers=3, Neurons=(64, 32, 16), Activation=relu\n",
            "Validation MAE: 4.2468\n",
            "Training with: LR=0.01, Layers=3, Neurons=(64, 32, 16), Activation=tanh\n",
            "Validation MAE: 8.0007\n",
            "Training with: LR=0.01, Layers=3, Neurons=(128, 64, 32), Activation=relu\n",
            "Validation MAE: 2.2430\n",
            "Training with: LR=0.01, Layers=3, Neurons=(128, 64, 32), Activation=tanh\n",
            "Validation MAE: 9.5432\n",
            "Training with: LR=0.001, Layers=1, Neurons=32, Activation=relu\n",
            "Validation MAE: 23.0247\n",
            "Training with: LR=0.001, Layers=1, Neurons=32, Activation=tanh\n",
            "Validation MAE: 48.7949\n",
            "Training with: LR=0.001, Layers=1, Neurons=64, Activation=relu\n",
            "Validation MAE: 22.0563\n",
            "Training with: LR=0.001, Layers=1, Neurons=64, Activation=tanh\n",
            "Validation MAE: 38.3053\n",
            "Training with: LR=0.001, Layers=1, Neurons=128, Activation=relu\n",
            "Validation MAE: 21.5438\n",
            "Training with: LR=0.001, Layers=1, Neurons=128, Activation=tanh\n",
            "Validation MAE: 23.9405\n",
            "Training with: LR=0.001, Layers=2, Neurons=(32, 32), Activation=relu\n",
            "Validation MAE: 16.7020\n",
            "Training with: LR=0.001, Layers=2, Neurons=(32, 32), Activation=tanh\n",
            "Validation MAE: 44.6306\n",
            "Training with: LR=0.001, Layers=2, Neurons=(64, 32), Activation=relu\n",
            "Validation MAE: 12.5456\n",
            "Training with: LR=0.001, Layers=2, Neurons=(64, 32), Activation=tanh\n",
            "Validation MAE: 43.3208\n",
            "Training with: LR=0.001, Layers=2, Neurons=(64, 64), Activation=relu\n",
            "Validation MAE: 11.8871\n",
            "Training with: LR=0.001, Layers=2, Neurons=(64, 64), Activation=tanh\n",
            "Validation MAE: 23.1031\n",
            "Training with: LR=0.001, Layers=2, Neurons=(128, 64), Activation=relu\n",
            "Validation MAE: 10.5463\n",
            "Training with: LR=0.001, Layers=2, Neurons=(128, 64), Activation=tanh\n",
            "Validation MAE: 24.2383\n",
            "Training with: LR=0.001, Layers=3, Neurons=(32, 32, 32), Activation=relu\n",
            "Validation MAE: 13.4366\n",
            "Training with: LR=0.001, Layers=3, Neurons=(32, 32, 32), Activation=tanh\n",
            "Validation MAE: 43.0792\n",
            "Training with: LR=0.001, Layers=3, Neurons=(64, 32, 16), Activation=relu\n",
            "Validation MAE: 13.5153\n",
            "Training with: LR=0.001, Layers=3, Neurons=(64, 32, 16), Activation=tanh\n",
            "Validation MAE: 59.6677\n",
            "Training with: LR=0.001, Layers=3, Neurons=(128, 64, 32), Activation=relu\n",
            "Validation MAE: 11.2359\n",
            "Training with: LR=0.001, Layers=3, Neurons=(128, 64, 32), Activation=tanh\n",
            "Validation MAE: 41.7696\n",
            "Training with: LR=0.0001, Layers=1, Neurons=32, Activation=relu\n",
            "Validation MAE: 74.3677\n",
            "Training with: LR=0.0001, Layers=1, Neurons=32, Activation=tanh\n",
            "Validation MAE: 83.2648\n",
            "Training with: LR=0.0001, Layers=1, Neurons=64, Activation=relu\n",
            "Validation MAE: 64.6809\n",
            "Training with: LR=0.0001, Layers=1, Neurons=64, Activation=tanh\n",
            "Validation MAE: 78.0123\n",
            "Training with: LR=0.0001, Layers=1, Neurons=128, Activation=relu\n",
            "Validation MAE: 57.8169\n",
            "Training with: LR=0.0001, Layers=1, Neurons=128, Activation=tanh\n",
            "Validation MAE: 68.6002\n",
            "Training with: LR=0.0001, Layers=2, Neurons=(32, 32), Activation=relu\n",
            "Validation MAE: 46.5568\n",
            "Training with: LR=0.0001, Layers=2, Neurons=(32, 32), Activation=tanh\n",
            "Validation MAE: 81.3950\n",
            "Training with: LR=0.0001, Layers=2, Neurons=(64, 32), Activation=relu\n",
            "Validation MAE: 44.4806\n",
            "Training with: LR=0.0001, Layers=2, Neurons=(64, 32), Activation=tanh\n",
            "Validation MAE: 81.6349\n",
            "Training with: LR=0.0001, Layers=2, Neurons=(64, 64), Activation=relu\n",
            "Validation MAE: 40.8886\n",
            "Training with: LR=0.0001, Layers=2, Neurons=(64, 64), Activation=tanh\n",
            "Validation MAE: 76.6320\n",
            "Training with: LR=0.0001, Layers=2, Neurons=(128, 64), Activation=relu\n",
            "Validation MAE: 35.7678\n",
            "Training with: LR=0.0001, Layers=2, Neurons=(128, 64), Activation=tanh\n",
            "Validation MAE: 72.7461\n",
            "Training with: LR=0.0001, Layers=3, Neurons=(32, 32, 32), Activation=relu\n",
            "Validation MAE: 42.2452\n",
            "Training with: LR=0.0001, Layers=3, Neurons=(32, 32, 32), Activation=tanh\n",
            "Validation MAE: 83.9106\n",
            "Training with: LR=0.0001, Layers=3, Neurons=(64, 32, 16), Activation=relu\n",
            "Validation MAE: 39.6340\n",
            "Training with: LR=0.0001, Layers=3, Neurons=(64, 32, 16), Activation=tanh\n",
            "Validation MAE: 86.4180\n",
            "Training with: LR=0.0001, Layers=3, Neurons=(128, 64, 32), Activation=relu\n",
            "Validation MAE: 28.0524\n",
            "Training with: LR=0.0001, Layers=3, Neurons=(128, 64, 32), Activation=tanh\n",
            "Validation MAE: 81.1182\n",
            "\n",
            "Best hyperparameters found:\n",
            "{'learning_rate': 0.01, 'num_layers': 3, 'neurons': (128, 64, 32), 'activation': 'relu'}\n",
            "Best Validation MAE: 2.2430\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76f23d32"
      },
      "source": [
        "## Retrain the best model\n",
        "\n",
        "### Subtask:\n",
        "Train the model with the selected hyperparameters on the entire training dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77ab13d3"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a new model using the `build_tunable_model` function with the `best_params` and train it on the entire dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "532582c2",
        "outputId": "7611c0f2-97e9-4fd7-ae67-1f2218ab87d5"
      },
      "source": [
        "# Define a new model with the best hyperparameters\n",
        "tuned_model = build_tunable_model(\n",
        "    num_layers=best_params['num_layers'],\n",
        "    neurons_per_layer=best_params['neurons'],\n",
        "    activation=best_params['activation'],\n",
        "    learning_rate=best_params['learning_rate']\n",
        ")\n",
        "\n",
        "# Train the model on the entire dataset (X and Y)\n",
        "print(\"Training the final model with best hyperparameters on the full dataset...\")\n",
        "tuned_model.fit(X, Y, epochs=20, batch_size=64)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the final model with best hyperparameters on the full dataset...\n",
            "Epoch 1/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 4746.7920 - mae: 44.9490\n",
            "Epoch 2/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 434.1216 - mae: 15.7311\n",
            "Epoch 3/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 373.1495 - mae: 14.4809\n",
            "Epoch 4/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 224.4123 - mae: 11.0847\n",
            "Epoch 5/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 86.7375 - mae: 6.7907\n",
            "Epoch 6/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 55.8162 - mae: 5.3030\n",
            "Epoch 7/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 29.9636 - mae: 3.9880\n",
            "Epoch 8/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 28.9228 - mae: 3.9097\n",
            "Epoch 9/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 38.4787 - mae: 4.3234\n",
            "Epoch 10/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 27.9754 - mae: 3.7702\n",
            "Epoch 11/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 17.7233 - mae: 3.1013\n",
            "Epoch 12/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 22.4593 - mae: 3.4187\n",
            "Epoch 13/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 46.1818 - mae: 4.6989\n",
            "Epoch 14/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 15.3015 - mae: 2.8788\n",
            "Epoch 15/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 86.5382 - mae: 6.2420\n",
            "Epoch 16/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 23.0960 - mae: 3.3793\n",
            "Epoch 17/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 20.6277 - mae: 3.0252\n",
            "Epoch 18/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 11.4570 - mae: 2.5067\n",
            "Epoch 19/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 12.0856 - mae: 2.5569\n",
            "Epoch 20/20\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 16.5633 - mae: 2.9657\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7e1b535d3a40>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f5f38dc"
      },
      "source": [
        "## Evaluate the final model\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the performance of the retrained model on a separate test set (if available) or using cross-validation to get a more robust estimate of its performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb691309"
      },
      "source": [
        "**Reasoning**:\n",
        "Generate a separate test dataset and evaluate the performance of the retrained model on it using MAE.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f652fb5",
        "outputId": "5eb8086e-e0f2-490e-b8ff-5c0fc6d4a09c"
      },
      "source": [
        "# Generate a new, separate test dataset\n",
        "X_test, Y_test = generate_data(5000) # Generate 5000 test samples\n",
        "\n",
        "# Evaluate the tuned model on the test dataset\n",
        "print(\"\\nEvaluating the tuned model on the test dataset...\")\n",
        "test_results = tuned_model.evaluate(X_test, Y_test, verbose=0)\n",
        "\n",
        "# Extract the MAE from the evaluation results\n",
        "# Assuming 'mae' was the metric used during compilation\n",
        "mae_on_test = test_results[1]\n",
        "\n",
        "# Print the MAE on the test data\n",
        "print(f\"Mean Absolute Error on test data: {mae_on_test:.4f}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating the tuned model on the test dataset...\n",
            "Mean Absolute Error on test data: 3.0325\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bbd6822"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The hyperparameter tuning process identified the best combination of hyperparameters from the defined search space as: Learning Rate: 0.01, Number of Layers: 3, Neurons per Layer: (128, 64, 32), and Activation Function: 'relu'.\n",
        "*   This best combination achieved the lowest validation MAE of 2.2430 during the tuning phase.\n",
        "*   After retraining the model with the best hyperparameters on the entire dataset, the evaluation on a separate test set resulted in a Mean Absolute Error of approximately 3.0325.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The difference between the validation MAE (2.2430) and the test MAE (3.0325) suggests potential overfitting during the tuning process or training on the full dataset. Further investigation into regularization techniques or cross-validation during tuning could be beneficial.\n",
        "*   Explore a wider range of hyperparameters, including different activation functions, optimizer types, and regularization parameters, to potentially achieve better generalization on unseen data.\n"
      ]
    }
  ]
}