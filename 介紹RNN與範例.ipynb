{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/twyeh/AI-in-education/blob/main/%E4%BB%8B%E7%B4%B9RNN%E8%88%87%E7%AF%84%E4%BE%8B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52bed39c",
      "metadata": {
        "id": "52bed39c"
      },
      "source": [
        "---\n",
        "\n",
        "## 介紹RNN神經網路的教材\n",
        "\n",
        "### 什麼是RNN？\n",
        "\n",
        "**遞歸神經網路（Recurrent Neural Network, RNN）**是一種深度學習模型，設計用於處理序列資料，如文字、語音或時間序列。它通過記住過去的輸入來預測未來的輸出，尤其適用於語言模型、語音辨識等領域[^1_1][^1_3]。\n",
        "\n",
        "### RNN的架構\n",
        "\n",
        "- **輸入層**：接收序列資料。\n",
        "- **隱藏層**：處理資料並記住過去的輸入。\n",
        "- **輸出層**：提供預測結果[^1_1][^1_3]。\n",
        "\n",
        "\n",
        "### RNN的運作方式\n",
        "\n",
        "1. **序列處理**：RNN一次處理一個序列元素，利用隱藏層記住過去的輸入以預測下一個元素[^1_1]。\n",
        "2. **時間反向傳播（BPTT）**：用於訓練RNN，計算誤差並調整權重[^1_1]。\n",
        "\n",
        "### RNN的類型\n",
        "\n",
        "- **一對一**：一個輸入對應一個輸出。\n",
        "- **一對多**：一個輸入對應多個輸出（如圖像字幕生成）。\n",
        "- **多對多**：多個輸入對應多個輸出（如語言翻譯）。\n",
        "- **多對一**：多個輸入對應一個輸出（如情緒分析）[^1_1]。\n",
        "\n",
        "\n",
        "### RNN的變體\n",
        "\n",
        "- **長短期記憶（LSTM）**：解決梯度消失問題，能記住更長的序列[^1_1][^1_3]。\n",
        "- **門控循環單元（GRU）**：簡化LSTM，選擇性保留記憶[^1_1]。\n",
        "\n",
        "\n",
        "### RNN的限制\n",
        "\n",
        "- **梯度爆炸與消失**：影響訓練穩定性[^1_1][^1_3]。\n",
        "- **訓練時間慢**：序列處理限制了效率[^1_1]。\n",
        "\n",
        "\n",
        "### 轉換程式的優勢\n",
        "\n",
        "轉換程式通過自我注意力機制，能更快地處理長序列，克服RNN的限制[^1_1]。\n",
        "\n",
        "<div style=\"text-align: center\">⁂</div>\n",
        "\n",
        "[^1_1]: https://aws.amazon.com/tw/what-is/recurrent-neural-network/\n",
        "\n",
        "[^1_2]: http://zake7749.github.io/2017/09/28/Sequence-to-Sequence-tutorial/\n",
        "\n",
        "[^1_3]: https://zh.wikipedia.org/zh-tw/循环神经网络\n",
        "\n",
        "[^1_4]: https://ithelp.ithome.com.tw/m/articles/10191404\n",
        "\n",
        "[^1_5]: https://brohrer.mcknote.com/zh-Hant/how_machine_learning_works/how_rnns_lstm_work.html\n",
        "\n",
        "[^1_6]: https://leemeng.tw/deep-learning-resources\n",
        "\n",
        "[^1_7]: https://ithelp.ithome.com.tw/articles/10193469\n",
        "\n",
        "[^1_8]: https://vocus.cc/article/67935451fd89780001da93d2\n",
        "\n",
        "---\n",
        "\n",
        "# RNN 的主要應用有哪些\n",
        "\n",
        "RNN的主要應用包括：\n",
        "\n",
        "- **自然語言處理（NLP）**：\n",
        "    - **語言建模**：預測下一個單詞或字符[^2_2][^2_8]。\n",
        "    - **機器翻譯**：將輸入語言轉換為目標語言[^2_1][^2_2]。\n",
        "    - **文本生成**：自動生成文本[^2_1][^2_2]。\n",
        "- **語音識別**：將聲音信號轉換為文字[^2_2][^2_4]。\n",
        "- **時間序列預測**：\n",
        "    - **股票預測**：根據歷史數據預測未來股價[^2_2]。\n",
        "    - **天氣預測**：利用歷史氣象數據預測未來天氣[^2_2]。\n",
        "- **其他應用**：\n",
        "    - **生成圖像描述**：描述圖片內容[^2_1]。\n",
        "    - **作曲**：生成音樂[^2_4]。\n",
        "\n",
        "<div style=\"text-align: center\">⁂</div>\n",
        "\n",
        "[^2_1]: https://easyai.tech/ai-definition/rnn/\n",
        "\n",
        "[^2_2]: https://blog.csdn.net/huidhsu/article/details/131478499\n",
        "\n",
        "[^2_3]: https://zh-tw.eitca.org/artificial-intelligence/eitc-ai-dlptfk-deep-learning-with-python-tensorflow-and-keras/recurrent-neural-networks/introduction-to-recurrent-neural-networks-rnn/examination-review-introduction-to-recurrent-neural-networks-rnn/what-is-the-main-advantage-of-using-recurrent-neural-networks-rnns-for-processing-sequential-data/\n",
        "\n",
        "[^2_4]: https://ithelp.ithome.com.tw/articles/10204741\n",
        "\n",
        "[^2_5]: https://aws.amazon.com/tw/what-is/recurrent-neural-network/\n",
        "\n",
        "[^2_6]: https://blog.csdn.net/qq_36187544/article/details/93052465\n",
        "\n",
        "[^2_7]: https://www.accucrazy.com/lstm-info/\n",
        "\n",
        "[^2_8]: https://cloud.tencent.com/developer/article/2299693\n",
        "\n",
        "---\n",
        "\n",
        "# RNN 和 CNN 在圖像描述中的結合如何提高性能\n",
        "\n",
        "RNN和CNN在圖像描述中的結合通過以下方式提高性能：\n",
        "\n",
        "1. **特征提取**：CNN從圖像中提取空間特征，提供了圖像的視覺信息[^3_3][^3_4]。\n",
        "2. **序列生成**：RNN利用CNN提取的特征生成描述文本，捕捉文本的時間依賴性和語法結構[^3_6][^3_7]。\n",
        "3. **互補優勢**：CNN的空間特征提取能力與RNN的序列處理能力互相補充，提高了圖像描述的準確性和連貫性[^3_5][^3_8]。\n",
        "\n",
        "這種結合使得模型能夠更好地理解圖像內容並生成自然語言描述。\n",
        "\n",
        "<div style=\"text-align: center\">⁂</div>\n",
        "\n",
        "[^3_1]: https://blog.csdn.net/MUKAMO/article/details/139428919\n",
        "\n",
        "[^3_2]: https://html.rhhz.net/buptjournal/html/20190622.htm\n",
        "\n",
        "[^3_3]: https://cloud.baidu.com/article/3321121\n",
        "\n",
        "[^3_4]: https://blog.csdn.net/linmingan/article/details/51800369\n",
        "\n",
        "[^3_5]: https://crad.ict.ac.cn/fileJSJYJYFZ/journal/article/jsjyjyfz/HTML/2021-09-1951.shtml\n",
        "\n",
        "[^3_6]: https://open.alipay.com/portal/forum/post/190701067\n",
        "\n",
        "[^3_7]: https://docs.feishu.cn/article/wiki/NZDGwqckPiO1iKkH37SczinZnPg\n",
        "\n",
        "[^3_8]: https://edu.51cto.com/file/76465.html\n",
        "\n",
        "---\n",
        "\n",
        "# CNN-RNN 架構的超參數如何影響其性能\n",
        "\n",
        "CNN-RNN架構的超參數對其性能有著重要影響。以下是主要的超參數及其影響：\n",
        "\n",
        "### CNN部分超參數\n",
        "\n",
        "- **卷積核大小和數量**：影響特征提取能力，較大的卷積核可以捕捉更大的特征，但可能增加計算覆雜度[^4_2][^4_6]。\n",
        "- **池化層參數**：如池化窗口大小和步長，影響數據維度和計算量[^4_2][^4_6]。\n",
        "- **全連接層神經元數量**：影響特征的綜合能力[^4_2]。\n",
        "- **學習率和批量大小**：影響訓練速度和穩定性[^4_3][^4_4]。\n",
        "\n",
        "\n",
        "### RNN部分超參數\n",
        "\n",
        "- **序列長度**：影響模型捕捉時序信息的能力[^4_1]。\n",
        "- **RNN類型（LSTM/GRU）**：LSTM更強大但計算成本更高，GRU則更輕量[^4_1]。\n",
        "- **隱藏層單元數量**：影響模型的表達能力，但過多可能導致過擬合[^4_1][^4_4]。\n",
        "\n",
        "\n",
        "### 訓練相關超參數\n",
        "\n",
        "- **叠代次數**：影響模型的訓練程度，過多可能導致過擬合[^4_1][^4_3]。\n",
        "\n",
        "這些超參數的選擇應根據任務需求、數據集特點和硬件資源進行調整，以優化模型性能[^4_1][^4_2][^4_3].\n",
        "\n",
        "<div style=\"text-align: center\">⁂</div>\n",
        "\n",
        "[^4_1]: https://blog.csdn.net/MUKAMO/article/details/139428919\n",
        "\n",
        "[^4_2]: https://jns.usst.edu.cn/html/2021/3/20210302.htm\n",
        "\n",
        "[^4_3]: https://www.cnblogs.com/bnuvincent/p/7570867.html\n",
        "\n",
        "[^4_4]: https://blog.csdn.net/lomodays207/article/details/86480948\n",
        "\n",
        "[^4_5]: https://vocus.cc/article/678c898cfd897800012d7158\n",
        "\n",
        "[^4_6]: https://cloud.baidu.com/article/1852841\n",
        "\n",
        "[^4_7]: https://github.com/scutan90/DeepLearning-500-questions/blob/master/ch14_超参数调整/第十四章_超参数调整.md\n",
        "\n",
        "---\n",
        "\n",
        "# 如何選擇CNN-RNN架構中的最佳超參數組合\n",
        "\n",
        "選擇CNN-RNN架構中的最佳超參數組合需要通過以下步驟：\n",
        "\n",
        "1. **定義搜索空間**：確定需要優化的超參數，如學習率、批量大小、叠代次數、卷積核大小、RNN類型（LSTM/GRU）、隱藏層單元數量等[^5_1][^5_2]。\n",
        "2. **使用搜索方法**：\n",
        "    - **網格搜索**：窮舉法列出所有可能的超參數組合，適合小規模參數空間[^5_2]。\n",
        "    - **隨機搜索**：從參數空間中隨機抽取組合，適合大規模參數空間[^5_2]。\n",
        "    - **自動調參工具**：如Hyperopt或Optuna，可以自動搜索最佳超參數組合[^5_7]。\n",
        "3. **評估性能**：使用驗證集評估每個超參數組合的性能，選擇表現最佳的組合[^5_6]。\n",
        "4. **叠代優化**：根據初始結果，進一步細化搜索範圍，提高精度[^5_2][^5_6]。\n",
        "5. **考慮硬件資源**：確保選擇的超參數組合在可用的硬件條件下能夠高效運行[^5_1][^5_3]。\n",
        "\n",
        "<div style=\"text-align: center\">⁂</div>\n",
        "\n",
        "[^5_1]: https://blog.csdn.net/MUKAMO/article/details/139428919\n",
        "\n",
        "[^5_2]: https://www.cnblogs.com/bnuvincent/p/7570867.html\n",
        "\n",
        "[^5_3]: https://blog.csdn.net/fengdu78/article/details/128878047\n",
        "\n",
        "[^5_4]: https://brohrer.mcknote.com/zh-Hant/how_machine_learning_works/how_convolutional_neural_networks_work.html\n",
        "\n",
        "[^5_5]: https://vocus.cc/article/678c898cfd897800012d7158\n",
        "\n",
        "[^5_6]: https://www.showmeai.tech/article-detail/218\n",
        "\n",
        "[^5_7]: https://claire-chang.com/2022/12/\n",
        "\n",
        "---\n",
        "\n",
        "# 如何在硬件資源有限的情況下優化CNN-RNN架構的超參數\n",
        "\n",
        "在硬件資源有限的情況下優化CNN-RNN架構的超參數，需要考慮以下策略：\n",
        "\n",
        "1. **模型輕量化**：\n",
        "    - 使用剪枝、量化、張量分解等方法減少模型參數量，降低計算覆雜度[^6_2]。\n",
        "    - 選擇較小的卷積核尺寸和較少的隱藏層單元數[^6_1][^6_3]。\n",
        "2. **批量大小調整**：\n",
        "    - 選擇合適的批量大小，以平衡訓練速度和模型性能。過小的批量大小可能導致梯度不穩定[^6_1]。\n",
        "3. **學習率和優化器選擇**：\n",
        "    - 選擇合適的學習率和優化器，如Adam或RMSprop，來提高訓練效率[^6_1][^6_5]。\n",
        "4. **硬件加速**：\n",
        "    - 利用GPU或FPGA等硬件加速器來加速計算[^6_4]。\n",
        "5. **超參數搜索方法**：\n",
        "    - 使用隨機搜索或網格搜索等方法，快速找到合適的超參數組合[^6_1][^6_5]。\n",
        "\n",
        "<div style=\"text-align: center\">⁂</div>\n",
        "\n",
        "[^6_1]: https://github.com/scutan90/DeepLearning-500-questions/blob/master/ch14_超参数调整/第十四章_超参数调整.md\n",
        "\n",
        "[^6_2]: http://cjc.ict.ac.cn/online/onlinepaper/gcp-2023112105635.pdf\n",
        "\n",
        "[^6_3]: https://blog.csdn.net/leonardotu/article/details/137651758\n",
        "\n",
        "[^6_4]: https://patents.google.com/patent/CN107704916A/zh\n",
        "\n",
        "[^6_5]: https://cloud.tencent.com/developer/article/1526918\n",
        "\n",
        "[^6_6]: https://blog.csdn.net/sgzqc/article/details/137521663\n",
        "\n",
        "[^6_7]: https://jeit.ac.cn/cn/article/doi/10.11999/JEIT231222?viewType=HTML\n",
        "\n",
        "[^6_8]: https://www.ccf.org.cn/upload/resources/file/2022/12/08/c77eea69c68b12d49e82af4dc021c67a.pdf\n",
        "\n",
        "---\n",
        "\n",
        "# 請利用RNN設計ㄧ個簡單的應用範例，並使用Python設計程式\n",
        "\n",
        "以下是一個使用RNN進行文字生成的簡單應用範例，並使用Python設計程式：\n",
        "\n",
        "### 程式碼"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "adfba155",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adfba155",
        "outputId": "2cfc6266-6a35-44e8-95ca-b5a3e4a133a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - loss: 2.3155\n",
            "Epoch 2/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 1.7734\n",
            "Epoch 3/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 1.3437\n",
            "Epoch 4/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.9989\n",
            "Epoch 5/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 0.7308\n",
            "Epoch 6/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 0.5333\n",
            "Epoch 7/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.3902\n",
            "Epoch 8/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.2829\n",
            "Epoch 9/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.2039\n",
            "Epoch 10/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1497\n",
            "Epoch 11/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1125\n",
            "Epoch 12/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.0852\n",
            "Epoch 13/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 0.0641\n",
            "Epoch 14/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.0483\n",
            "Epoch 15/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.0369\n",
            "Epoch 16/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.0288\n",
            "Epoch 17/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.0230\n",
            "Epoch 18/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0185\n",
            "Epoch 19/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0150\n",
            "Epoch 20/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - loss: 0.0122\n",
            "Epoch 21/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 0.0100\n",
            "Epoch 22/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - loss: 0.0083\n",
            "Epoch 23/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 0.0069\n",
            "Epoch 24/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 0.0059\n",
            "Epoch 25/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.0051\n",
            "Epoch 26/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.0044\n",
            "Epoch 27/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.0039\n",
            "Epoch 28/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - loss: 0.0035\n",
            "Epoch 29/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.0031\n",
            "Epoch 30/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.0028\n",
            "Seed: 道格看見珍\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 298ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
            "Generated Text: 道格看見珍。珍看見小點。小點看見道格。看見。小。看見道格。看見。小。看見道格。看見。小。看見道格。看見。小。看\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import SimpleRNN, Dense, Activation\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# 資料準備：簡單的文字序列\n",
        "text = \"道格看見珍。珍看見小點。小點看見道格。\"\n",
        "chars = sorted(list(set(text)))  # 字符集\n",
        "char_to_index = {char: idx for idx, char in enumerate(chars)}\n",
        "index_to_char = {idx: char for idx, char in enumerate(chars)}\n",
        "\n",
        "# 超參數設定\n",
        "seq_length = 5  # 序列長度\n",
        "hidden_units = 50  # 隱藏層單元數\n",
        "batch_size = 32  # 批量大小\n",
        "\n",
        "# 資料處理：將文字轉換為數字序列\n",
        "data_X, data_y = [], []\n",
        "for i in range(len(text) - seq_length):\n",
        "    seq_in = text[i:i + seq_length]\n",
        "    seq_out = text[i + seq_length]\n",
        "    data_X.append([char_to_index[char] for char in seq_in])\n",
        "    data_y.append(char_to_index[seq_out])\n",
        "\n",
        "data_X = np.array(data_X)\n",
        "data_y = to_categorical(data_y, num_classes=len(chars))\n",
        "\n",
        "# 建立模型\n",
        "model = Sequential([\n",
        "    SimpleRNN(hidden_units, input_shape=(seq_length, len(chars)), unroll=True),\n",
        "    Dense(len(chars)),\n",
        "    Activation(\"softmax\")\n",
        "])\n",
        "\n",
        "# 編譯模型\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.01))\n",
        "\n",
        "# 訓練模型\n",
        "data_X_onehot = np.zeros((len(data_X), seq_length, len(chars)))\n",
        "for i, sequence in enumerate(data_X):\n",
        "    for j, char_idx in enumerate(sequence):\n",
        "        data_X_onehot[i, j, char_idx] = 1\n",
        "\n",
        "model.fit(data_X_onehot, data_y, batch_size=batch_size, epochs=30)\n",
        "\n",
        "# 測試模型：生成文字\n",
        "seed_text = \"道格看見珍\"\n",
        "print(\"Seed:\", seed_text)\n",
        "for _ in range(50):  # 預測50個字符\n",
        "    seed_seq = np.zeros((1, seq_length, len(chars)))\n",
        "    for i, char in enumerate(seed_text[-seq_length:]):\n",
        "        seed_seq[0, i, char_to_index[char]] = 1\n",
        "\n",
        "    pred_idx = np.argmax(model.predict(seed_seq))\n",
        "    pred_char = index_to_char[pred_idx]\n",
        "    seed_text += pred_char\n",
        "\n",
        "print(\"Generated Text:\", seed_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0612512",
      "metadata": {
        "id": "b0612512"
      },
      "source": [
        "### 程式說明\n",
        "\n",
        "1. **資料準備**：\n",
        "    - 將輸入文字轉換為數字序列，並生成對應的輸出字符。\n",
        "    - 使用`char_to_index`和`index_to_char`進行字符與索引的映射。\n",
        "2. **模型架構**：\n",
        "    - 使用`SimpleRNN`作為隱藏層，並添加全連接層和Softmax激活函數進行分類。\n",
        "3. **訓練模型**：\n",
        "    - 將輸入序列轉換為One-Hot編碼格式。\n",
        "    - 使用交叉熵損失函數和Adam優化器進行訓練。\n",
        "4. **文字生成**：\n",
        "    - 根據種子文本進行預測，逐步生成新字符並拼接到文本中。\n",
        "\n",
        "此範例展示了如何利用RNN生成基於上下文的文字序列，適合初學者理解RNN的基本原理。\n",
        "\n",
        "<div style=\"text-align: center\">⁂</div>\n",
        "\n",
        "[^7_1]: https://brohrer.mcknote.com/zh-Hant/how_machine_learning_works/how_rnns_lstm_work.html\n",
        "\n",
        "[^7_2]: https://ithelp.ithome.com.tw/articles/10193469\n",
        "\n",
        "[^7_3]: https://ithelp.ithome.com.tw/articles/10321948?sc=rss.iron\n",
        "\n",
        "[^7_4]: https://blog.csdn.net/weixin_45012204/article/details/124646289\n",
        "\n",
        "[^7_5]: https://aws.amazon.com/tw/what-is/recurrent-neural-network/\n",
        "\n",
        "[^7_6]: https://www.youtube.com/watch?v=eBsxhX-81rQ\n",
        "\n",
        "[^7_7]: https://makerpro.cc/2019/07/self-learning-series6-of-ai-rnn/\n",
        "\n",
        "---\n",
        "\n",
        "# 使用Python撰寫ㄧ個使用RNN的範例\n",
        "\n",
        "以下是一個使用Python撰寫的簡單RNN範例，利用Keras和TensorFlow進行手寫數字辨識（MNIST資料集）：\n",
        "\n",
        "### 範例程式"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "42f8ad55",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42f8ad55",
        "outputId": "b2028ffc-d578-4834-c201-e6284a523ffd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "Epoch 1/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.6376 - loss: 1.0563 - val_accuracy: 0.8657 - val_loss: 0.4413\n",
            "Epoch 2/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.8822 - loss: 0.3905 - val_accuracy: 0.9263 - val_loss: 0.2633\n",
            "Epoch 3/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9135 - loss: 0.2951 - val_accuracy: 0.9318 - val_loss: 0.2411\n",
            "Epoch 4/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.9307 - loss: 0.2448 - val_accuracy: 0.9377 - val_loss: 0.2203\n",
            "Epoch 5/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.9359 - loss: 0.2235 - val_accuracy: 0.9471 - val_loss: 0.1894\n",
            "Epoch 6/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9404 - loss: 0.2061 - val_accuracy: 0.9479 - val_loss: 0.1877\n",
            "Epoch 7/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.9451 - loss: 0.1901 - val_accuracy: 0.9384 - val_loss: 0.2099\n",
            "Epoch 8/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.9469 - loss: 0.1862 - val_accuracy: 0.9442 - val_loss: 0.1954\n",
            "Epoch 9/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.9484 - loss: 0.1809 - val_accuracy: 0.9502 - val_loss: 0.1736\n",
            "Epoch 10/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.9503 - loss: 0.1733 - val_accuracy: 0.9504 - val_loss: 0.1731\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9393 - loss: 0.2056\n",
            "Test accuracy: 0.95\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import SimpleRNN, Dense\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# 載入MNIST資料集\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# 資料預處理\n",
        "X_train = X_train.reshape(-1, 28, 28) / 255.0\n",
        "X_test = X_test.reshape(-1, 28, 28) / 255.0\n",
        "\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# 建立RNN模型\n",
        "model = Sequential([\n",
        "    SimpleRNN(units=50, input_shape=(28, 28), unroll=True),\n",
        "    Dense(10),\n",
        "    Activation(\"softmax\")\n",
        "])\n",
        "\n",
        "# 編譯模型\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# 訓練模型\n",
        "model.fit(X_train, y_train, batch_size=32, epochs=10, validation_data=(X_test, y_test))\n",
        "\n",
        "# 測試模型\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test accuracy: {accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "830970b4",
      "metadata": {
        "id": "830970b4"
      },
      "source": [
        "### 範例說明\n",
        "\n",
        "1. **資料載入**：使用Keras的`mnist.load_data()`載入MNIST資料集。\n",
        "2. **資料預處理**：將圖像資料正規化到0到1之間，並將標籤轉換為One-Hot編碼。\n",
        "3. **模型建立**：使用`SimpleRNN`作為隱藏層，後接全連接層和Softmax激活函數進行分類。\n",
        "4. **模型訓練**：使用交叉熵損失函數和Adam優化器進行訓練。\n",
        "5. **模型評估**：在測試集上評估模型的準確率。\n",
        "\n",
        "<div style=\"text-align: center\">⁂</div>\n",
        "\n",
        "[^8_1]: https://ithelp.ithome.com.tw/articles/10193469\n",
        "\n",
        "[^8_2]: https://ithelp.ithome.com.tw/articles/10289229\n",
        "\n",
        "[^8_3]: https://mofanpy.com/tutorials/machine-learning/tensorflow/RNN2\n",
        "\n",
        "[^8_4]: https://mmchiou.gitbooks.io/ai_challenge_taiwan_dt_2018-private/content/rnn/rnn-professor-li-hong-yi-21-1.html\n",
        "\n",
        "[^8_5]: https://www.books.com.tw/products/0010913521\n",
        "\n",
        "[^8_6]: https://www.tenlong.com.tw/products/9789860776775\n",
        "\n",
        "[^8_7]: https://www.kaggle.com/code/rimmelasghar/rnn-implementation-from-scratch-in-python\n",
        "\n",
        "[^8_8]: https://github.com/erhwenkuo/deep-learning-with-keras-notebooks/blob/master/1.c-lstm-learn-alphabetic-seq.ipynb\n",
        "\n",
        "---\n",
        "\n",
        "# 如何在Python中實現一個使用LSTM的RNN模型\n",
        "\n",
        "以下是使用Python實現一個使用LSTM的RNN模型的步驟和程式碼：\n",
        "\n",
        "### 步驟\n",
        "\n",
        "1. **安裝必要的庫**：\n",
        "    - `pip install tensorflow` 或 `pip install torch`，視乎你使用TensorFlow還是PyTorch。\n",
        "2. **資料預處理**：\n",
        "    - 將序列資料轉換為適合LSTM的格式，通常是三維張量（樣本數、時間步、特徵維度）。\n",
        "3. **建立LSTM模型**：\n",
        "    - 使用TensorFlow或PyTorch建立一個包含LSTM層的模型。\n",
        "4. **訓練模型**：\n",
        "    - 使用適合的優化器和損失函數訓練模型。\n",
        "\n",
        "### 範例程式（TensorFlow）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b5da660d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5da660d",
        "outputId": "705adc55-6868-4a28-d3a8-0f159dc5748b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - loss: 0.4196\n",
            "Epoch 2/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.3002\n",
            "Epoch 3/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2243\n",
            "Epoch 4/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1410\n",
            "Epoch 5/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1027\n",
            "Epoch 6/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1269 \n",
            "Epoch 7/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1336\n",
            "Epoch 8/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1237\n",
            "Epoch 9/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1060\n",
            "Epoch 10/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1112 \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7f0db7f22650>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "# 假設資料\n",
        "data = np.random.rand(100, 10, 1)  # 100個樣本，每個樣本10個時間步，1個特徵\n",
        "labels = np.random.rand(100, 1)  # 100個標籤\n",
        "\n",
        "# 建立模型\n",
        "model = Sequential([\n",
        "    LSTM(50, input_shape=(10, 1)),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# 編譯模型\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# 訓練模型\n",
        "model.fit(data, labels, epochs=10, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8ccf59d",
      "metadata": {
        "id": "d8ccf59d"
      },
      "source": [
        "### 範例程式（PyTorch）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9ff82f60",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ff82f60",
        "outputId": "b9ff1904-6dfc-4215-a462-068269d9f713"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.8486247062683105\n",
            "Epoch 2, Loss: 0.8471788763999939\n",
            "Epoch 3, Loss: 0.8458098769187927\n",
            "Epoch 4, Loss: 0.8445150852203369\n",
            "Epoch 5, Loss: 0.8432900309562683\n",
            "Epoch 6, Loss: 0.8421288132667542\n",
            "Epoch 7, Loss: 0.8410249948501587\n",
            "Epoch 8, Loss: 0.8399732112884521\n",
            "Epoch 9, Loss: 0.8389686346054077\n",
            "Epoch 10, Loss: 0.8380071520805359\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 假設資料\n",
        "data = torch.randn(100, 10, 1)  # 100個樣本，每個樣本10個時間步，1個特徵\n",
        "labels = torch.randn(100, 1)  # 100個標籤\n",
        "\n",
        "class SimpleLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleLSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        out = self.fc(out[:, -1, :])  # 取最後一個時間步的輸出\n",
        "        return out\n",
        "\n",
        "# 建立模型\n",
        "input_size = 1\n",
        "hidden_size = 32\n",
        "output_size = 1\n",
        "model = SimpleLSTM(input_size, hidden_size, output_size)\n",
        "\n",
        "# 訓練模型\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(10):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(data)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95e5dfc1",
      "metadata": {
        "id": "95e5dfc1"
      },
      "source": [
        "這些範例展示了如何使用TensorFlow和PyTorch建立和訓練LSTM模型。\n",
        "\n",
        "<div style=\"text-align: center\">⁂</div>\n",
        "\n",
        "[^9_1]: https://blog.csdn.net/weixin_36873225/article/details/132342145\n",
        "\n",
        "[^9_2]: https://www.cnblogs.com/TS86/p/18610973\n",
        "\n",
        "[^9_3]: https://cloud.tencent.com/developer/article/2417195\n",
        "\n",
        "[^9_4]: https://blog.csdn.net/qq_41698317/article/details/142303859\n",
        "\n",
        "[^9_5]: https://vocus.cc/article/6693787dfd89780001ac301a\n",
        "\n",
        "[^9_6]: https://blog.51cto.com/u_16175517/6811199\n",
        "\n",
        "---\n",
        "\n",
        "# 如何在Python中使用Keras實現LSTM模型\n",
        "\n",
        "以下是使用Python和Keras實現LSTM模型的步驟和程式碼：\n",
        "\n",
        "### 步驟\n",
        "\n",
        "1. **安裝必要的庫**：\n",
        "    - `pip install tensorflow` 或 `pip install keras`。\n",
        "2. **資料準備**：\n",
        "    - 將資料轉換為適合LSTM的格式，通常是三維張量（樣本數、時間步、特徵維度）。\n",
        "3. **建立LSTM模型**：\n",
        "    - 使用Keras的`Sequential`或`Model`API建立模型。\n",
        "4. **編譯模型**：\n",
        "    - 指定損失函數、優化器和評估指標。\n",
        "5. **訓練模型**：\n",
        "    - 使用`fit()`方法訓練模型。\n",
        "\n",
        "### 範例程式"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f727048f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f727048f",
        "outputId": "57bef943-edf3-4254-ee76-ac44b098ec67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.4013\n",
            "Epoch 2/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2501\n",
            "Epoch 3/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1610 \n",
            "Epoch 4/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1044 \n",
            "Epoch 5/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0887\n",
            "Epoch 6/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0855\n",
            "Epoch 7/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0973\n",
            "Epoch 8/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0878\n",
            "Epoch 9/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0766\n",
            "Epoch 10/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0847\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "import numpy as np\n",
        "\n",
        "# 假設資料\n",
        "data = np.random.rand(100, 10, 1)  # 100個樣本，每個樣本10個時間步，1個特徵\n",
        "labels = np.random.rand(100, 1)  # 100個標籤\n",
        "\n",
        "# 建立模型\n",
        "model = Sequential([\n",
        "    LSTM(50, input_shape=(10, 1)),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# 編譯模型\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "# 訓練模型\n",
        "model.fit(data, labels, epochs=10, batch_size=32)\n",
        "\n",
        "# 保存模型\n",
        "model.save('lstm_model.h5')\n",
        "model.save('lstm_model.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aab93060",
      "metadata": {
        "id": "aab93060"
      },
      "source": [
        "### 範例說明\n",
        "\n",
        "1. **資料準備**：假設有100個樣本，每個樣本10個時間步，1個特徵。\n",
        "2. **模型建立**：使用`Sequential`建立一個包含LSTM層和全連接層的模型。\n",
        "3. **模型編譯**：指定均方差損失函數和Adam優化器。\n",
        "4. **模型訓練**：使用`fit()`方法訓練模型。\n",
        "5. **模型保存**：使用`save()`方法保存模型。\n",
        "\n",
        "<div style=\"text-align: center\">⁂</div>\n",
        "\n",
        "[^10_1]: https://blog.csdn.net/weixin_53250004/article/details/129631635\n",
        "\n",
        "[^10_2]: https://hsin-life.com/archives/2328\n",
        "\n",
        "[^10_3]: https://blog.51cto.com/u_16213689/9200465\n",
        "\n",
        "[^10_4]: https://github.com/apachecn/ml-mastery-zh/blob/master/docs/dlts/time-series-prediction-lstm-recurrent-neural-networks-python-keras.md\n",
        "\n",
        "[^10_5]: https://blog.csdn.net/weixin_44052055/article/details/122432705\n",
        "\n",
        "[^10_6]: https://blog.51cto.com/u_16175471/7071855\n",
        "\n",
        "[^10_7]: https://cloud.tencent.com/developer/article/1705651\n",
        "\n",
        "[^10_8]: https://github.com/GlassyWing/bi-lstm-crf"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Olpt2ie_-Y6D"
      },
      "id": "Olpt2ie_-Y6D",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}