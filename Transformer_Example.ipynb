{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMT/3BOzf8fLi5vvYtygssL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/twyeh/AI-in-education/blob/main/Transformer_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests bs4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n76inFkfnyMd",
        "outputId": "bef659b6-1eb3-46b9-a327-853e1e790ca4"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.11/dist-packages (0.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from bs4) (4.13.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->bs4) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->bs4) (4.13.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import numpy"
      ],
      "metadata": {
        "id": "yHb-0aSSoeLb"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Web scraping configuration\n",
        "BASE_URLS = [\"https://en.wikipedia.org/wiki/Artificial_intelligence\",\n",
        "             \"https://www.nytimes.com/section/technology\",\n",
        "             \"https://en.wikipedia.org/wiki/Transformer\"]  # Example URLs\n",
        "VOCAB_SIZE = 30000\n",
        "MAX_LEN = 200\n",
        "EMBED_DIM = 128\n",
        "NUM_HEADS = 4\n",
        "FF_DIM = 256\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 3"
      ],
      "metadata": {
        "id": "AnWv-oeRo-XX"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_article(url):\n",
        "    \"\"\"Scrape and clean article text from a webpage\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Remove non-content elements\n",
        "        for element in soup(['script', 'style', 'nav', 'footer']):\n",
        "            element.decompose()\n",
        "\n",
        "        # Extract and clean text\n",
        "        text = ' '.join([p.get_text() for p in soup.find_all('p')])\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping {url}: {e}\")\n",
        "        return \"\""
      ],
      "metadata": {
        "id": "dAgu0puNpJ3f"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scrape training data from multiple sources\n",
        "print(\"Scraping training data...\")\n",
        "corpus = []\n",
        "for url in BASE_URLS:\n",
        "    article = scrape_article(url)\n",
        "    if article:\n",
        "        corpus.append(article)\n",
        "#corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHmPuibWpRJA",
        "outputId": "a6a39cbd-5246-4f5a-e7ae-d6cf9758548f"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping training data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create text vectorization layer\n",
        "text_vectorization = layers.TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=MAX_LEN + 1  # +1 for target sequence\n",
        ")"
      ],
      "metadata": {
        "id": "JX279LH5pWtq"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare dataset\n",
        "text_dataset = tf.data.Dataset.from_tensor_slices(corpus)\n",
        "text_vectorization.adapt(text_dataset.batch(64))\n"
      ],
      "metadata": {
        "id": "EifOvhQYpbod"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_lm_dataset(text_batch):\n",
        "    text_vectorized = text_vectorization(text_batch)\n",
        "    return text_vectorized[:, :-1], text_vectorized[:, 1:]  # Input and target"
      ],
      "metadata": {
        "id": "5yzQfHSrpiG3"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = text_dataset \\\n",
        "    .batch(BATCH_SIZE) \\\n",
        "    .map(prepare_lm_dataset, num_parallel_calls=tf.data.AUTOTUNE) \\\n",
        "    .prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "hIpvFrbxpnHy"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer components (same as original)\n",
        "class PositionalEncoding(layers.Layer):\n",
        "    def __init__(self, max_len, embed_dim):\n",
        "        super().__init__()\n",
        "        self.pos_emb = layers.Embedding(input_dim=max_len, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        positions = tf.range(start=0, limit=tf.shape(x)[1], delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        return x + positions"
      ],
      "metadata": {
        "id": "FKcoM_bbpqwt"
      },
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential([\n",
        "            layers.Dense(ff_dim, activation=\"relu\"),\n",
        "            layers.Dense(embed_dim),\n",
        "        ])\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n"
      ],
      "metadata": {
        "id": "dBVggG4YpvL_"
      },
      "execution_count": 175,
      "outputs": []
    },
    {
      "source": [
        "# Build language model\n",
        "inputs = layers.Input(shape=(MAX_LEN,))\n",
        "x = layers.Embedding(VOCAB_SIZE, EMBED_DIM)(inputs)\n",
        "x = PositionalEncoding(MAX_LEN, EMBED_DIM)(x)\n",
        "x = TransformerBlock(EMBED_DIM, NUM_HEADS, FF_DIM)(x, training=False) # Pass training=False here\n",
        "outputs = layers.Dense(VOCAB_SIZE, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "MPtd1BW_ZEXe"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gb84cNPCmTkP",
        "outputId": "0fdfe632-56bc-41de-d777-b15d685903ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        }
      ],
      "source": [
        "# Train model\n",
        "history = model.fit(\n",
        "    dataset,\n",
        "    epochs=EPOCHS\n",
        ")"
      ]
    },
    {
      "source": [
        "# Generate text example\n",
        "import numpy\n",
        "def generate_text(prompt, length=50):\n",
        "    output_string={}\n",
        "    tokens = text_vectorization([prompt]).numpy()[0]\n",
        "    generated_tokens = []  # Store generated word tokens\n",
        "    for _ in range(length):\n",
        "        pred = model.predict(tokens[-MAX_LEN:].reshape(1, -1), verbose=0)\n",
        "        next_token = tf.argmax(pred[0, -1, :]).numpy()\n",
        "\n",
        "        # Check if next_token represents a word (alphanumeric)\n",
        "        token_string = text_vectorization.get_vocabulary()[next_token.item()]\n",
        "        if token_string.isalpha():  # Check if token is alphanumeric\n",
        "            generated_tokens.append(next_token)\n",
        "\n",
        "        tokens = numpy.append(tokens, next_token.item())\n",
        "\n",
        "    # Convert generated tokens back to words\n",
        "    generated_words = [text_vectorization.get_vocabulary()[token] for token in generated_tokens]\n",
        "    output_string = ' '.join(generated_words)\n",
        "    return output_string\n",
        "   # return generated_words"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "pmPCHteiepjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text('is', length=50)"
      ],
      "metadata": {
        "id": "RmfTQ5YwZ0XG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}