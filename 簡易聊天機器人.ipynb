{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNpbXSaA4KoFsNbcy+YS09V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/twyeh/AI-in-education/blob/main/%E7%B0%A1%E6%98%93%E8%81%8A%E5%A4%A9%E6%A9%9F%E5%99%A8%E4%BA%BA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://www.fs-greentek.com/tw/article/%E6%A0%B8%E8%83%BD-vs-%E7%B6%A0%E8%83%BD-%E7%9A%84%E8%BE%AF%E8%AB%96-%E9%83%BD%E5%BF%BD%E7%95%A5%E4%BA%86%E6%9C%80%E6%A0%B9%E6%9C%AC%E6%80%A7%E7%9A%84%E5%B0%88%E5%88%A9%E6%AC%8A%E5%92%8C%E6%8A%80%E8%A1%93%E5%95%8F%E9%A1%8C.html\"  # 將此處替換為文章的網址\n",
        "response = requests.get(url)\n",
        "article_content = response.text\n",
        "\n",
        "with open(\"article.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(article_content)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "2UIhBUqWXxis"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# 定義模型\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=1000, output_dim=100),\n",
        "    tf.keras.layers.LSTM(units=100),\n",
        "    tf.keras.layers.Dense(100, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "# 編譯模型\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "#  Create a sample dataset for demonstration - REPLACE THIS WITH YOUR ACTUAL DATA\n",
        "# Assuming your data is sequences of integers with a maximum value of 999\n",
        "num_samples = 1000\n",
        "sequence_length = 50  # 設定 sequence_length\n",
        "dataset = np.random.randint(0, 1000, size=(num_samples, sequence_length))\n",
        "\n",
        "# Create sample labels - REPLACE THIS WITH YOUR ACTUAL LABELS\n",
        "# Assuming you have 100 classes\n",
        "labels = np.random.randint(0, 100, size=(num_samples,))\n",
        "labels = tf.keras.utils.to_categorical(labels, num_classes=100)  # One-hot encode labels\n",
        "\n",
        "# 訓練模型\n",
        "model.fit(dataset, labels, epochs=20)  # Pass labels to the fit method\n",
        "\n",
        "# 使用模型進行聊天\n",
        "def chat(text):\n",
        "    # 將輸入文字轉換為模型輸入格式\n",
        "    input_data = preprocess_text(text)\n",
        "\n",
        "    # 使用模型預測輸出\n",
        "    output = model.predict(input_data.reshape(1, -1))  # 調整輸入資料形狀\n",
        "\n",
        "    # 將輸出轉換為文字\n",
        "    response = generate_text(output)\n",
        "\n",
        "    return response\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocesses the input text for the model.\n",
        "\n",
        "    Args:\n",
        "        text: The input text string.\n",
        "\n",
        "    Returns:\n",
        "        A NumPy array representing the preprocessed text.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Tokenization: Split the text into individual words or tokens.\n",
        "    tokens = text.lower().split()  # Simple split by spaces; consider more advanced tokenizers\n",
        "\n",
        "    # 2. Create a vocabulary: Map each unique token to a numerical index.\n",
        "    # (Assuming you have a vocabulary from your dataset; replace with your actual logic)\n",
        "    vocabulary = {\"<PAD>\": 0, \"<UNK>\": 1}  # Initialize with padding and unknown tokens\n",
        "    for token in tokens:\n",
        "        if token not in vocabulary:\n",
        "            vocabulary[token] = len(vocabulary)\n",
        "\n",
        "    # 3. Convert tokens to numerical indices: Replace each token with its index.\n",
        "    indexed_tokens = [vocabulary.get(token, vocabulary[\"<UNK>\"]) for token in tokens]\n",
        "\n",
        "    # 4. Padding: Ensure all sequences have the same length.\n",
        "    # (Assuming your model expects sequences of length 'sequence_length'; adjust as needed)\n",
        "    padded_sequence = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        [indexed_tokens], maxlen=sequence_length, padding=\"post\", truncating=\"post\"\n",
        "    )\n",
        "\n",
        "    return padded_sequence[0]  # Return the padded sequence as a NumPy array\n",
        "\n",
        "def generate_text(output):\n",
        "    \"\"\"\n",
        "    Generates text from the model's output.\n",
        "\n",
        "    Args:\n",
        "        output: The model's output (e.g., probabilities over vocabulary).\n",
        "\n",
        "    Returns:\n",
        "        The generated text string.\n",
        "    \"\"\"\n",
        "    # 1. Get the predicted class index (e.g., word index with highest probability)\n",
        "    predicted_index = np.argmax(output)\n",
        "\n",
        "    # 2. Reverse your vocabulary mapping to get the word from the index\n",
        "    # Assuming you have 'vocabulary' defined elsewhere in your code\n",
        "    vocabulary_size = model.layers[0].get_config()['vocabulary_size']  # 獲取詞彙表大小\n",
        "    reverse_vocabulary = {index: word for index, word in enumerate(vocabulary)}  # 建立 reverse_vocabulary\n",
        "\n",
        "    # 3. Get the predicted word\n",
        "    predicted_word = reverse_vocabulary.get(predicted_index, \"<UNK>\")\n",
        "\n",
        "    # 4. You might want to add logic to generate longer responses:\n",
        "    #    - Sample from the probability distribution instead of just taking argmax.\n",
        "    #    - Use beam search or other decoding strategies for more coherent text.\n",
        "\n",
        "    return predicted_word  # Return the generated word (or a longer response)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_KIBcKjbcwb",
        "outputId": "c6f8fec2-df82-4af1-9064-ffccf29ffba3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 45ms/step - accuracy: 0.0098 - loss: 4.6055\n",
            "Epoch 2/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - accuracy: 0.0774 - loss: 4.5784\n",
            "Epoch 3/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - accuracy: 0.1345 - loss: 4.5481\n",
            "Epoch 4/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - accuracy: 0.0727 - loss: 4.4457\n",
            "Epoch 5/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - accuracy: 0.1701 - loss: 4.3072\n",
            "Epoch 6/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - accuracy: 0.1882 - loss: 4.0761\n",
            "Epoch 7/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step - accuracy: 0.2188 - loss: 3.7174\n",
            "Epoch 8/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step - accuracy: 0.2698 - loss: 3.4076\n",
            "Epoch 9/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - accuracy: 0.2920 - loss: 3.0480\n",
            "Epoch 10/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - accuracy: 0.4324 - loss: 2.6106\n",
            "Epoch 11/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - accuracy: 0.5570 - loss: 2.3302\n",
            "Epoch 12/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 62ms/step - accuracy: 0.6162 - loss: 2.0011\n",
            "Epoch 13/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.6801 - loss: 1.7035\n",
            "Epoch 14/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.7895 - loss: 1.3554\n",
            "Epoch 15/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - accuracy: 0.8693 - loss: 1.0697\n",
            "Epoch 16/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - accuracy: 0.9072 - loss: 0.9214\n",
            "Epoch 17/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - accuracy: 0.9448 - loss: 0.6946\n",
            "Epoch 18/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 79ms/step - accuracy: 0.9726 - loss: 0.5439\n",
            "Epoch 19/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.9840 - loss: 0.4436\n",
            "Epoch 20/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.9939 - loss: 0.3349\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# 定義模型\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=1000, output_dim=100),\n",
        "    tf.keras.layers.LSTM(units=100),\n",
        "    tf.keras.layers.Dense(100, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "# 編譯模型\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "#  Create a sample dataset for demonstration - REPLACE THIS WITH YOUR ACTUAL DATA\n",
        "# Assuming your data is sequences of integers with a maximum value of 999\n",
        "num_samples = 1000\n",
        "sequence_length = 50  # 設定 sequence_length\n",
        "dataset = np.random.randint(0, 1000, size=(num_samples, sequence_length))\n",
        "\n",
        "# Create sample labels - REPLACE THIS WITH YOUR ACTUAL LABELS\n",
        "# Assuming you have 100 classes\n",
        "labels = np.random.randint(0, 100, size=(num_samples,))\n",
        "labels = tf.keras.utils.to_categorical(labels, num_classes=100)  # One-hot encode labels\n",
        "\n",
        "# 訓練模型\n",
        "model.fit(dataset, labels, epochs=20)  # Pass labels to the fit method\n",
        "\n",
        "# 使用模型進行聊天\n",
        "def chat(text):\n",
        "    # 將輸入文字轉換為模型輸入格式\n",
        "    input_data = preprocess_text(text)\n",
        "\n",
        "    # 使用模型預測輸出\n",
        "    output = model.predict(input_data.reshape(1, -1))  # 調整輸入資料形狀\n",
        "\n",
        "    # 將輸出轉換為文字\n",
        "    response = generate_text(output)\n",
        "\n",
        "    return response\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocesses the input text for the model.\n",
        "\n",
        "    Args:\n",
        "        text: The input text string.\n",
        "\n",
        "    Returns:\n",
        "        A NumPy array representing the preprocessed text.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Tokenization: Split the text into individual words or tokens.\n",
        "    tokens = text.lower().split()  # Simple split by spaces; consider more advanced tokenizers\n",
        "\n",
        "    # 2. Create a vocabulary: Map each unique token to a numerical index.\n",
        "    # (Assuming you have a vocabulary from your dataset; replace with your actual logic)\n",
        "    vocabulary = {\"<PAD>\": 0, \"<UNK>\": 1}  # Initialize with padding and unknown tokens\n",
        "    for token in tokens:\n",
        "        if token not in vocabulary:\n",
        "            vocabulary[token] = len(vocabulary)\n",
        "\n",
        "    # 3. Convert tokens to numerical indices: Replace each token with its index.\n",
        "    indexed_tokens = [vocabulary.get(token, vocabulary[\"<UNK>\"]) for token in tokens]\n",
        "\n",
        "    # 4. Padding: Ensure all sequences have the same length.\n",
        "    # (Assuming your model expects sequences of length 'sequence_length'; adjust as needed)\n",
        "    padded_sequence = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        [indexed_tokens], maxlen=sequence_length, padding=\"post\", truncating=\"post\"\n",
        "    )\n",
        "\n",
        "    return padded_sequence[0]  # Return the padded sequence as a NumPy array\n",
        "\n",
        "def generate_text(output):\n",
        "    \"\"\"\n",
        "    Generates text from the model's output.\n",
        "\n",
        "    Args:\n",
        "        output: The model's output (e.g., probabilities over vocabulary).\n",
        "\n",
        "    Returns:\n",
        "        The generated text string.\n",
        "    \"\"\"\n",
        "    # 1. Get the predicted class index (e.g., word index with highest probability)\n",
        "    predicted_index = np.argmax(output)\n",
        "\n",
        "    # 2. Reverse your vocabulary mapping to get the word from the index\n",
        "    # Assuming you have 'vocabulary' defined elsewhere in your code\n",
        "    # vocabulary_size = model.layers[0].get_config()['vocabulary_size']  # 獲取詞彙表大小 - This line is causing the error\n",
        "\n",
        "    # Instead, get vocabulary size from your vocabulary directly\n",
        "    vocabulary_size = len(vocabulary) # vocabulary is defined in preprocess_text\n",
        "\n",
        "    reverse_vocabulary = {index: word for index, word in enumerate(vocabulary)}  # 建立 reverse_vocabulary #But vocabulary is not in scope\n",
        "\n",
        "    # 3. Get the predicted word\n",
        "    predicted_word = reverse_vocabulary.get(predicted_index, \"<UNK>\")\n",
        "\n",
        "    # 4. You might want to add logic to generate longer responses:\n",
        "    #    - Sample from the probability distribution instead of just taking argmax.\n",
        "    #    - Use beam search or other decoding strategies for more coherent text.\n",
        "\n",
        "    return predicted_word  # Return the generated word (or a longer response)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBlBSYIecVTG",
        "outputId": "f477e216-0531-4ad9-f478-ec55ad5661b4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 46ms/step - accuracy: 0.0187 - loss: 4.6054\n",
            "Epoch 2/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - accuracy: 0.0648 - loss: 4.5784\n",
            "Epoch 3/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.0338 - loss: 4.5413\n",
            "Epoch 4/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 47ms/step - accuracy: 0.0650 - loss: 4.4251\n",
            "Epoch 5/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 54ms/step - accuracy: 0.1784 - loss: 4.3554\n",
            "Epoch 6/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 78ms/step - accuracy: 0.1395 - loss: 4.1456\n",
            "Epoch 7/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - accuracy: 0.1640 - loss: 3.9509\n",
            "Epoch 8/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - accuracy: 0.1787 - loss: 3.6854\n",
            "Epoch 9/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 50ms/step - accuracy: 0.2739 - loss: 3.4014\n",
            "Epoch 10/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step - accuracy: 0.3236 - loss: 3.1120\n",
            "Epoch 11/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - accuracy: 0.3977 - loss: 2.7545\n",
            "Epoch 12/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - accuracy: 0.4913 - loss: 2.4063\n",
            "Epoch 13/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.5977 - loss: 2.0691\n",
            "Epoch 14/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.6851 - loss: 1.6937\n",
            "Epoch 15/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - accuracy: 0.7793 - loss: 1.3981\n",
            "Epoch 16/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - accuracy: 0.8627 - loss: 1.1328\n",
            "Epoch 17/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - accuracy: 0.9140 - loss: 0.8639\n",
            "Epoch 18/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 56ms/step - accuracy: 0.9395 - loss: 0.6923\n",
            "Epoch 19/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step - accuracy: 0.9644 - loss: 0.5128\n",
            "Epoch 20/20\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - accuracy: 0.9894 - loss: 0.4153\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed_text=\"為什麼歐洲\"\n",
        "chat(seed_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "krqcR7U7aVny",
        "outputId": "8e2f97f1-1293-47de-dad0-c29ed1e6876a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'vocabulary' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-9d1eb23955d5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mseed_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"為什麼歐洲\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-bba82eb7669d>\u001b[0m in \u001b[0;36mchat\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# 將輸出轉換為文字\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-bba82eb7669d>\u001b[0m in \u001b[0;36mgenerate_text\u001b[0;34m(output)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;31m# Instead, get vocabulary size from your vocabulary directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m     \u001b[0mvocabulary_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# vocabulary is defined in preprocess_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mreverse_vocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m  \u001b[0;31m# 建立 reverse_vocabulary #But vocabulary is not in scope\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'vocabulary' is not defined"
          ]
        }
      ]
    },
    {
      "source": [
        "# 定義全域詞彙表\n",
        "vocabulary = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # ... (程式碼不變) ...\n",
        "\n",
        "    # 更新詞彙表\n",
        "    global vocabulary  # 宣告使用全域詞彙表\n",
        "    for token in tokens:\n",
        "        if token not in vocabulary:\n",
        "            vocabulary[token] = len(vocabulary)\n",
        "\n",
        "    # ... (程式碼不變) ...\n",
        "\n",
        "def generate_text(output):\n",
        "    # ... (程式碼不變) ...\n",
        "\n",
        "    # 使用全域詞彙表\n",
        "    global vocabulary\n",
        "    vocabulary_size = len(vocabulary)\n",
        "    reverse_vocabulary = {index: word for index, word in enumerate(vocabulary)}\n",
        "\n",
        "    # ... (程式碼不變) ..."
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Mabmt1TZeDlS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}